{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artanebibi/datascience/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHArRpW87Jd7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkDpbqjYOlc-"
      },
      "outputs": [],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTyW63I3ltGM"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPf8moULT5kS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import selenium as sns\n",
        "from queue import Queue\n",
        "import threading\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PebcqASoCil8"
      },
      "source": [
        " <h1> STOCK NAMES AND CODES </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av9-kV-vYl15"
      },
      "outputs": [],
      "source": [
        "stocks = [\n",
        "    \"Komercijalna banka AD Skopje\", \"Alkaloid AD Skopje\", \"DS Smith AD Skopje\",\n",
        "    \"Fersped AD Skopje\", \"Granit AD Skopje\", \"Hoteli Metropol Ohrid\",\n",
        "    \"Internesnel Hotels AD Skopje\", \"Makedonijaturist AD Skopje\", \"Makosped AD Skopje\",\n",
        "    \"Makoteks AD Skopje\", \"Makpetrol AD Skopje\",  \"Makstil AD Skopje\", \"Replek AD Skopje\",\n",
        "    \"RZ Inter-Transsped AD Skopje\", \"RZ Uslugi AD Skopje\", \"Skopski Pazar AD Skopje\", \"Stopanska banka AD Bitola\",\n",
        "    \"Teteks AD Tetovo\", \"TTK Banka AD Skopje\", \"Tutunski kombinat AD Prilep\",\n",
        "    \"Vitaminka AD Prilep\", \"VV Tikves AD Kavadarci\", \"Zito Luks AD Skopje\",\n",
        "    \"ZK Pelagonija AD Bitola\", \"Ading AD  Skopje\", \"Agromehanika AD Skopje\",\n",
        "    \"Angropromet Tikvesanka AD Kavadarci\",\n",
        "    \"ArcelorMittal (HRM) AD Skopje - dolgorocno suspendirano od kotacija\",\n",
        "    \"Automakedonija AD Skopje\", \"BIM AD Sveti Nikole\",\n",
        "    \"Blagoj Tufanov AD Radovis - dolgorocno suspendirano od kotacija\",\n",
        "    \"Cementarnica USJE AD Skopje\", \"Centralna kooperativna banka AD Skopje\",\n",
        "    \"Debarski Bani –Capa AD  Debar\", \"Dimko Mitrev AD Veles\", \"Evropa AD Skopje\",\n",
        "    \"Fabrika Karpos AD Skopje\", \"FAKOM AD Skopje\", \"Fruktal Mak AD Skopje\",\n",
        "    \"Fustelarko Borec AD Bitola\", \"FZC 11-ti OKTOMVRI AD Kumanovo\",\n",
        "    \"GD-TIKVES AD Kavadarci\", \"Geras Cunev Konfekcija AD Strumica\",\n",
        "    \"Geras Cunev Trgovija AD Strumica\", \"Grozd AD Strumica\", \"GTC AD Skopje\",\n",
        "    \"Interpromet AD Tetovo\", \"Klanica so ladilnik AD Strumica\",\n",
        "    \"Kristal 1923 AD Veles\", \"Liberti AD Skopje\", \"Lotarija na Makedonija AD Skopje\",\n",
        "    \"Makedonija osiguruvane AD Skopje - Viena Insurens Grup\",\n",
        "    \"Makedonski Telekom AD – Skopje\", \"Makpromet AD Stip\",\n",
        "    \"Mermeren kombinat AD Prilep\", \"Moda AD Sveti Nikole\", \"MZT Pumpi AD Skopje\",\n",
        "    \"Nemetali Ograzden AD Strumica\", \"NLB Banka AD Skopje\",\n",
        "    \"Nova Stokovna kuka AD Strumica\", \"OILKO KDA Skopje\", \"OKTA AD Skopje\",\n",
        "    \"Oranzerii Hamzali Strumica\", \"Patnicki soobrakaj Transkop AD Bitola\",\n",
        "    \"Pekabesko AD Kadino Ilinden\", \"Pelisterka AD Skopje\", \"Popova Kula AD Demir Kapija\",\n",
        "    \"Prilepska pivarnica AD Prilep\", \"Rade Koncar- Aparatna tehnika AD Skopje\",\n",
        "    \"Rudnici Banani AD Skopje\", \"RZ Ekonomika AD Skopje\",\n",
        "    \"RZ Tehnicka Kontrola AD Skopje\", \"Sigurnosno staklo AD Prilep\",\n",
        "    \"Sileks AD Kratovo\", \"Slavej  AD Skopje\", \"Sovremen dom AD Prilep\",\n",
        "    \"Stokopromet AD Skopje\", \"Stopanska banka AD Skopje\",\n",
        "    \"Strumicko pole s. Vasilevo\", \"Tajmiste AD Kicevo - dolgorocno suspendirano od kotacija\",\n",
        "    \"TEAL AD  Tetovo\", \"Tehnokomerc AD Skopje\",\n",
        "    \"Trgotekstil Maloprodazba AD Skopje\", \"Triglav Osiguruvane AD Skopje\",\n",
        "    \"Trudbenik AD Ohrid - dolgorocno suspendirano od kotacija\",\n",
        "    \"Ugotur AD Skopje\", \"UNI Banka AD Skopje\", \"Vabtek MZT AD Skopje\",\n",
        "    \"Veteks AD Veles\", \"ZAS AD Skopje\", \"Zito Karaorman AD Kicevo\",\n",
        "    \"Zito Polog AD Tetovo\"\n",
        "]\n",
        "\n",
        "\n",
        "def filter_stockname():\n",
        "    stock_names = []\n",
        "    for company in stocks:\n",
        "        parts = company.split()\n",
        "        # get rid of city names for better search\n",
        "        filtered = [part for part in parts if\n",
        "                    part not in {\"AD\", \"Skopje\", \"Bitola\", \"Tetovo\", \"Prilep\", \"Kavadarci\", \"Ohrid\", \"Veles\",\n",
        "                                 \"Kumanovo\", \"Sveti\", \"Nikole\", \"Debar\", \"Radovis\", \"Stip\", \"Ilinden\", \"Kicevo\",\n",
        "                                 \"Strumica\", \" - dolgorocno suspendirano od kotacija\"}]\n",
        "        stock_names.append(\" \".join(filtered))\n",
        "\n",
        "    return stock_names\n",
        "\n",
        "\n",
        "def getStockCode_db():\n",
        "\n",
        "  stock_codes = [\n",
        "    \"KMB\", \"ALK\", \"KOMU\",\n",
        "    \"FERS\", \"GRNT\", \"MPOL\",\n",
        "    \"INHO\", \"MTUR\", \"MKSD\",\n",
        "    \"MAKS\", \"MPT\", \"STIL\", \"REPL\",\n",
        "    \"RZIT\", \"RZUS\", \"SPAZ\", \"SBT\",\n",
        "    \"TETE\", \"TTK\", \"TKPR\",\n",
        "    \"VITA\", \"TKVS\", \"ZILU\", \"ZPKO\", \"ADIN\", \"AMEH\", \"APTK\", \"RZLE\", \"AUMK\", \"BIM\",\n",
        "    \"BLTU\", \"USJE\", \"CKB\", \"DEBA\", \"DIMI\", \"EVRO\", \"KPSS\", \"FAKM\", \"KONZ\", \"FUBT\",\n",
        "    \"CEVI\", \"TIKV\", \"GECK\", \"GECT\", \"GRZD\", \"GTC\", \"INPR\", \"KLST\", \"BGOR\", \"RZLV\",\n",
        "    \"LOTO\", \"KJUBI\", \"TEL\", \"MAKP\", \"MERM\", \"MODA\", \"MZPU\", \"NEME\", \"TNB\", \"NOSK\",\n",
        "    \"OILK\", \"OKTA\", \"ORAN\", \"TRPS\", \"PKB\", \"LOZP\", \"POPK\", \"PPIV\", \"RADE\", \"BANA\",\n",
        "    \"RZEK\", \"RZTK\", \"SSPR\", \"SIL\", \"SLAV\", \"SDOM\", \"STOK\", \"STB\", \"SPOL\", \"TAJM\",\n",
        "    \"TEAL\", \"TEHN\", \"TSMP\", \"VROS\", \"TRDB\", \"RZUG\", \"UNI\", \"MZHE\", \"VTKS\", \"ZAS\",\n",
        "    \"ZKAR\", \"ZPOG\"\n",
        "  ]\n",
        "\n",
        "\n",
        "  return stock_codes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnPPmOaiaTGq"
      },
      "outputs": [],
      "source": [
        "def create_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    return webdriver.Chrome(options=options)\n",
        "\n",
        "\n",
        "class SeleniumDriverPool:\n",
        "    def __init__(self, max_size=5):\n",
        "        self.pool = Queue(maxsize=max_size)\n",
        "        self.lock = threading.Lock()\n",
        "        for _ in range(max_size):\n",
        "            driver = create_driver()\n",
        "            self.pool.put(driver)\n",
        "\n",
        "    def acquire_driver(self):\n",
        "        with self.lock:\n",
        "            return self.pool.get()\n",
        "\n",
        "    def release_driver(self, driver):\n",
        "        with self.lock:\n",
        "            self.pool.put(driver)\n",
        "\n",
        "    def close_all_drivers(self):\n",
        "        while not self.pool.empty():\n",
        "            driver = self.pool.get()\n",
        "            driver.quit()\n",
        "\n",
        "\n",
        "pool = SeleniumDriverPool(max_size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqVcNd_BuN4x"
      },
      "source": [
        "<H1> SCRAPING <i>kapital.mk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK5jgAVXUk2f"
      },
      "outputs": [],
      "source": [
        "def split_list(lst):\n",
        "    mid = len(lst) // 2\n",
        "    return lst[:mid], lst[mid:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DhXo7E9EpHXw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "def scrape_kapitalMK():\n",
        "    stock_dataframes = {}\n",
        "\n",
        "    try:\n",
        "        filtered_stocks = filter_stockname()\n",
        "        stock_codes = getStockCode_db()\n",
        "    except NameError:\n",
        "        print(\"Error: Required functions (filter_stockname, getStockCode_db) are not defined.\")\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        pool1 = SeleniumDriverPool(max_size=20)\n",
        "    except NameError:\n",
        "        print(\"Error: SeleniumDriverPool is not defined.\")\n",
        "        return {}\n",
        "\n",
        "    for stock_name, stock_code in zip(filtered_stocks, stock_codes):\n",
        "        data = []\n",
        "        try:\n",
        "            driver = pool1.acquire_driver()\n",
        "            i = 1\n",
        "\n",
        "            while True:\n",
        "                driver.get(f'https://kapital.mk/page/{i}/?s={stock_name}')\n",
        "                time.sleep(2)  # Ensures page loads completely\n",
        "                html_content = driver.page_source\n",
        "\n",
        "                if 'Error 404!' in html_content or 'Sorry, your search did not match any entries.' in html_content:\n",
        "                    break\n",
        "\n",
        "                article_list = driver.find_elements(By.CSS_SELECTOR, \".mvp-blog-story-list li\")\n",
        "\n",
        "                for tag in article_list:\n",
        "                    a_tag = tag.find_element(By.CSS_SELECTOR, \"a:nth-child(1)\")\n",
        "                    link = a_tag.get_attribute('href')\n",
        "                    print(link)\n",
        "\n",
        "                    try:\n",
        "                        nd = pool1.acquire_driver()\n",
        "                        nd.set_page_load_timeout(15)  # Increase timeout to 15 seconds\n",
        "                        nd.get(link)\n",
        "                        time.sleep(2)\n",
        "\n",
        "                        date = nd.find_element(By.CSS_SELECTOR, 'time')\n",
        "                        date_published = datetime.fromisoformat(date.get_attribute('datetime')).strftime(\"%d.%m.%Y\")\n",
        "                        print(date_published)\n",
        "\n",
        "                        title = nd.find_element(By.CSS_SELECTOR, \"h1.entry-title\").text\n",
        "                        print(title)\n",
        "\n",
        "                        post = nd.find_element(By.CSS_SELECTOR, \".mvp-post-soc-out\").text\n",
        "                        actual_post = post.split('ПОВРЗАНИ ТЕМИ:')[0]\n",
        "\n",
        "                        if len(actual_post) < 5000:  # Skip long reads\n",
        "                            data.append({'date': date_published, 'title': title, 'text': actual_post, 'link': link, 'from': 'kapital.mk'})\n",
        "                        else:\n",
        "                            print(f\"Skipping article {link} due to excessive length.\")\n",
        "\n",
        "                    except TimeoutException:\n",
        "                        print(f\"Timeout occurred while processing link {link}, skipping article.\")\n",
        "                    except WebDriverException as e:\n",
        "                        print(f\"Error occurred while processing link {link}: {e}\")\n",
        "                    finally:\n",
        "                        pool1.release_driver(nd)\n",
        "\n",
        "                i += 1\n",
        "\n",
        "        except WebDriverException as e:\n",
        "            print(f\"Error occurred for stock {stock_name} on page {i}: {e}\")\n",
        "        finally:\n",
        "            pool1.release_driver(driver)\n",
        "\n",
        "        stock_dataframes[stock_code] = pd.DataFrame(data)\n",
        "\n",
        "    pool1.close_all_drivers()\n",
        "    return stock_dataframes\n",
        "\n",
        "news2 = scrape_kapitalMK()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X9AVmQikv91"
      },
      "source": [
        "<h1>SCRAPING <i>biznisinfo.mk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmIfcSSDiViZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from datetime import datetime\n",
        "import threading\n",
        "from queue import Queue\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def scrape_biznisinfoMK():\n",
        "    stock_dataframes = {}\n",
        "    filtered_stocks, filtered_stocks_2 = split_list(filter_stockname())\n",
        "    stock_codes, stock_codes_2 = split_list(getStockCode_db())\n",
        "    pool2 = SeleniumDriverPool(max_size=20)\n",
        "    while True:\n",
        "\n",
        "        for stock_name, stock_code in zip(filtered_stocks, stock_codes):\n",
        "            data = []\n",
        "\n",
        "            try:\n",
        "                driver = pool2.acquire_driver()\n",
        "                i = 1\n",
        "\n",
        "                while True:\n",
        "                    driver.get(f'https://biznisinfo.mk/page/{i}/?s={stock_name}')\n",
        "                    html_content = driver.page_source\n",
        "\n",
        "                    if 'Упсссс... Error 404' in html_content or 'Нема резултати за вашето пребарување' in html_content:\n",
        "                        break\n",
        "\n",
        "                    article_list = driver.find_elements(By.CSS_SELECTOR, \".td_module_10\")\n",
        "                    print(f'Title: {article_list}')\n",
        "                    for tag in article_list:\n",
        "                        a_tag = tag.find_element(By.CSS_SELECTOR, \"div:nth-child(1) > a:nth-child(1)\")\n",
        "                        link = a_tag.get_attribute('href')\n",
        "                        print(link)\n",
        "\n",
        "                        try:\n",
        "                            nd = pool2.acquire_driver()\n",
        "                            nd.get(link)\n",
        "\n",
        "                            date = nd.find_element(By.CSS_SELECTOR, 'time')\n",
        "                            date_published = datetime.strptime(date.text, \"%d/%m/%Y\").strftime(\"%d.%m.%Y\")\n",
        "                            print()\n",
        "                            print(date_published)\n",
        "                            print()\n",
        "\n",
        "                            title = nd.find_element(By.CSS_SELECTOR, \"h1.entry-title\").text\n",
        "                            print(title)\n",
        "\n",
        "                            post = nd.find_element(By.CSS_SELECTOR, \".td-main-content\").text\n",
        "                            actual_post = post.split('Можеби ќе ве интересира и...')[0]\n",
        "\n",
        "                            # if stock_name in actual_post:\n",
        "                            data.append({'date': date_published, 'title': title, 'text': actual_post, 'link': link, 'from': 'biznisinfo.mk'})\n",
        "\n",
        "                        except WebDriverException as e:\n",
        "                            print(f\"Error occurred while processing link {link}: {e}\")\n",
        "\n",
        "                        finally:\n",
        "                            pool2.release_driver(nd)\n",
        "\n",
        "                    i += 1\n",
        "\n",
        "            except WebDriverException as e:\n",
        "                print(f\"Error occurred for stock {stock_name} on page {i}: {e}\")\n",
        "\n",
        "            finally:\n",
        "                pool2.release_driver(driver)\n",
        "\n",
        "            stock_dataframes[stock_code] = pd.DataFrame(data)\n",
        "\n",
        "        break\n",
        "\n",
        "    pool2.close_all_drivers()\n",
        "    return stock_dataframes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "news1 = scrape_biznisinfoMK()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pcy0EC9dya45"
      },
      "outputs": [],
      "source": [
        "for stock_name, df in news2.items():\n",
        "        print(f\"Data for {stock_name}:\")\n",
        "        print(df.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlOjqFv3b1QK"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ6F5p8c55wy"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import io\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "\n",
        "class WasabiClient:\n",
        "    def __init__(self):\n",
        "        access_key = 'A02ZUKINN1XDCJKJYI0D'\n",
        "        secret_key = 'ZB0o901t7NOFzmkmOU34HDFg2K1Pdrr0d3XKRhjh'\n",
        "        self.bucket = 'mkdstocks'\n",
        "        self.s3_client = boto3.client(\n",
        "            's3',\n",
        "            endpoint_url='https://s3.eu-central-2.wasabisys.com',\n",
        "            aws_access_key_id=access_key,\n",
        "            aws_secret_access_key=secret_key,\n",
        "        )\n",
        "\n",
        "    def does_key_exist(self, key: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if a given key exists in the Wasabi bucket.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.s3_client.head_object(Bucket=self.bucket, Key=key)\n",
        "            return True\n",
        "        except ClientError as e:\n",
        "            if e.response['Error']['Code'] == '404':\n",
        "                return False\n",
        "            else:\n",
        "                # print(f\"Unexpected error when checking key existence: {e}\")\n",
        "                return False\n",
        "\n",
        "    def create_articles(self, code: str, df: pd.DataFrame):\n",
        "        cloud_key = f\"Articles/{code}.csv\"\n",
        "\n",
        "        try:\n",
        "            csv_buffer = io.StringIO()\n",
        "            df.to_csv(csv_buffer, index=False)\n",
        "            csv_buffer.seek(0)\n",
        "            self.s3_client.put_object(Bucket=self.bucket, Key=cloud_key, Body=csv_buffer.getvalue())\n",
        "            print(f\"Successfully appended and uploaded data for {code}.\")\n",
        "        except ClientError as e:\n",
        "            print(f\"Error uploading data: {e}\")\n",
        "\n",
        "    def update_or_create_articles(self, code: str, new_df: pd.DataFrame):\n",
        "        cloud_key = f\"Articles/{code}.csv\"\n",
        "        try:\n",
        "            existing_df = None\n",
        "\n",
        "            if self.does_key_exist(cloud_key):\n",
        "                existing_file = self.s3_client.get_object(Bucket=self.bucket, Key=cloud_key)\n",
        "                existing_df = pd.read_csv(io.StringIO(existing_file['Body'].read().decode('utf-8')))\n",
        "\n",
        "            combined_df = pd.concat([new_df, existing_df]).drop_duplicates() if existing_df is not None else new_df\n",
        "            csv_buffer = io.StringIO()\n",
        "            combined_df.to_csv(csv_buffer, index=False)\n",
        "            csv_buffer.seek(0)\n",
        "            self.s3_client.put_object(Bucket=self.bucket, Key=cloud_key, Body=csv_buffer.getvalue())\n",
        "            print(f\"Successfully appended and uploaded data for {code}.\")\n",
        "        except ClientError as e:\n",
        "            print(f\"Error uploading data: {e}\")\n",
        "\n",
        "    def fetch_articles(self, code: str):\n",
        "        cloud_key = f\"Articles/{code}.csv\"\n",
        "        if self.does_key_exist(cloud_key):\n",
        "            file_response = self.s3_client.get_object(Bucket=self.bucket, Key=cloud_key)\n",
        "            file_content = file_response['Body'].read().decode('utf-8')  # Fix typo\n",
        "            return pd.read_csv(io.StringIO(file_content))\n",
        "        else :\n",
        "            print(f\"Error fetching data: {cloud_key}\")\n",
        "            return None\n",
        "\n",
        "def initialize_wasabi_client():\n",
        "    return WasabiClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFF3xUc_QfI8"
      },
      "source": [
        "# *MODELS AND SENTIMENT ANALYSIS*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y6Nr4U7qB51"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer, pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer_finbert = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model_finbert = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\",\n",
        "                                                                   output_hidden_states=False)\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "# for translation\n",
        "tokenizer_translator = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mk-en')\n",
        "model_translator = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-mk-en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIlvcYg-6_bI"
      },
      "outputs": [],
      "source": [
        "wasabi_client = WasabiClient()\n",
        "\n",
        "def split_text_into_chunks(text, max_length=512):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        if len(' '.join(current_chunk + [word])) <= max_length:\n",
        "            current_chunk.append(word)\n",
        "        else:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "for stock_name, df in news2.items():\n",
        "    final_df = []\n",
        "    for index, row in df.iterrows():\n",
        "        # Translate title\n",
        "        macedonian_title = row['title'].replace('„', '').replace('“', '')\n",
        "        encoded_input = tokenizer_translator(macedonian_title, return_tensors='pt')\n",
        "        translated = model_translator.generate(**encoded_input)\n",
        "        english_title = tokenizer_translator.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "        # Translate content in chunks\n",
        "        macedonian_content = row['text']\n",
        "        chunks = split_text_into_chunks(macedonian_content, max_length=512)\n",
        "        translated_chunks = []\n",
        "        sentiment_labels = []\n",
        "\n",
        "        for chunk in chunks:\n",
        "            encoded_input2 = tokenizer_translator(chunk, return_tensors='pt')\n",
        "            translated2 = model_translator.generate(**encoded_input2)\n",
        "            translated_chunk = tokenizer_translator.decode(translated2[0], skip_special_tokens=True)\n",
        "            translated_chunks.append(translated_chunk)\n",
        "\n",
        "            # Sentiment analysis for each chunk\n",
        "            result = sentiment_analyzer(translated_chunk)\n",
        "            sentiment_labels.append(result[0]['label'])\n",
        "\n",
        "        english_content = ' '.join(translated_chunks)\n",
        "        final_sentiment = ', '.join(sentiment_labels)\n",
        "\n",
        "        # Append to final_df\n",
        "        final_df.append({\n",
        "            'date': row['date'],\n",
        "            'title': english_title,\n",
        "            'content': english_content,\n",
        "            'sentiment': final_sentiment,\n",
        "            'link': row['link']\n",
        "        })\n",
        "\n",
        "        print(f\"FOR STOCK: {stock_name}\")\n",
        "        print(f\"Sentiment: {final_sentiment}\\n\")\n",
        "\n",
        "    # Save the translations to WasabiClient\n",
        "    final_for_stock = pd.DataFrame(final_df)\n",
        "    wasabi_client.update_or_create_articles(stock_name, final_for_stock)\n",
        "    print(\"File successfully uploaded to Wasabi:\", stock_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD5fwZfZL3157IbxvvM6du",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}