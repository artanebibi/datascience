{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artanebibi/datascience/blob/main/Sentiment_Analysis_biznisinfo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHArRpW87Jd7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "id": "VkDpbqjYOlc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTyW63I3ltGM"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPf8moULT5kS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import selenium as sns\n",
        "from queue import Queue\n",
        "import threading\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtmcxawrOsxK"
      },
      "source": [
        "Freezing the un-needed parameters from fin-bert model to fasten up the training process. While focusing only on fine-tuning the already trained finBERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PebcqASoCil8"
      },
      "source": [
        " <h1> STOCK NAMES AND CODES </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av9-kV-vYl15"
      },
      "outputs": [],
      "source": [
        "stocks = [\n",
        "    # \"Komercijalna banka AD Skopje\", \"Alkaloid AD Skopje\", \"DS Smith AD Skopje\",\n",
        "    # \"Fersped AD Skopje\", \"Granit AD Skopje\", \"Hoteli Metropol Ohrid\",\n",
        "    # \"Internesnel Hotels AD Skopje\", \"Makedonijaturist AD Skopje\",\n",
        "    # \"Makosped AD Skopje\",\n",
        "\n",
        "    \"Makoteks AD Skopje\", \"Makpetrol AD Skopje\",\n",
        "    \"Makstil AD Skopje\", \"Replek AD Skopje\", \"RZ Inter-Transsped AD Skopje\",\n",
        "    \"RZ Uslugi AD Skopje\", \"Skopski Pazar AD Skopje\", \"Stopanska banka AD Bitola\",\n",
        "    \"Teteks AD Tetovo\", \"TTK Banka AD Skopje\", \"Tutunski kombinat AD Prilep\",\n",
        "    \"Vitaminka AD Prilep\", \"VV Tikves AD Kavadarci\", \"Zito Luks AD Skopje\",\n",
        "    \"ZK Pelagonija AD Bitola\", \"Ading AD  Skopje\", \"Agromehanika AD Skopje\",\n",
        "    \"Angropromet Tikvesanka AD Kavadarci\",\n",
        "    \"ArcelorMittal (HRM) AD Skopje - dolgorocno suspendirano od kotacija\",\n",
        "    \"Automakedonija AD Skopje\", \"BIM AD Sveti Nikole\",\n",
        "    \"Blagoj Tufanov AD Radovis - dolgorocno suspendirano od kotacija\",\n",
        "    \"Cementarnica USJE AD Skopje\", \"Centralna kooperativna banka AD Skopje\",\n",
        "    \"Debarski Bani –Capa AD  Debar\", \"Dimko Mitrev AD Veles\",\n",
        "\n",
        "    # \"Fabrika Karpos AD Skopje\", \"FAKOM AD Skopje\", \"Fruktal Mak AD Skopje\",\n",
        "    # \"Fustelarko Borec AD Bitola\", \"FZC 11-ti OKTOMVRI AD Kumanovo\",\n",
        "    # \"GD-TIKVES AD Kavadarci\", \"Geras Cunev Konfekcija AD Strumica\",\n",
        "    # \"Geras Cunev Trgovija AD Strumica\", \"Grozd AD Strumica\", \"GTC AD Skopje\",\n",
        "    # \"Interpromet AD Tetovo\", \"Klanica so ladilnik AD Strumica\",\n",
        "    # \"Kristal 1923 AD Veles\", \"Liberti AD Skopje\", \"Lotarija na Makedonija AD Skopje\",\n",
        "    # \"Makedonija osiguruvane AD Skopje - Viena Insurens Grup\",\n",
        "    # \"Makedonski Telekom AD – Skopje\", \"Makpromet AD Stip\",\n",
        "    # \"Mermeren kombinat AD Prilep\", \"Moda AD Sveti Nikole\", \"MZT Pumpi AD Skopje\",\n",
        "    # \"Nemetali Ograzden AD Strumica\", \"NLB Banka AD Skopje\",\n",
        "    # \"Nova Stokovna kuka AD Strumica\", \"OILKO KDA Skopje\", \"OKTA AD Skopje\",\n",
        "    # \"Oranzerii Hamzali Strumica\", \"Patnicki soobrakaj Transkop AD Bitola\",\n",
        "    # \"Pekabesko AD Kadino Ilinden\", \"Pelisterka AD Skopje\", \"Popova Kula AD Demir Kapija\",\n",
        "    # \"Prilepska pivarnica AD Prilep\", \"Rade Koncar- Aparatna tehnika AD Skopje\",\n",
        "    # \"Rudnici Banani AD Skopje\", \"RZ Ekonomika AD Skopje\",\n",
        "    # \"RZ Tehnicka Kontrola AD Skopje\", \"Sigurnosno staklo AD Prilep\",\n",
        "    # \"Sileks AD Kratovo\", \"Slavej  AD Skopje\", \"Sovremen dom AD Prilep\",\n",
        "    # \"Stokopromet AD Skopje\", \"Stopanska banka AD Skopje\",\n",
        "    # \"Strumicko pole s. Vasilevo\", \"Tajmiste AD Kicevo - dolgorocno suspendirano od kotacija\",\n",
        "    # \"TEAL AD  Tetovo\", \"Tehnokomerc AD Skopje\",\n",
        "    # \"Trgotekstil Maloprodazba AD Skopje\", \"Triglav Osiguruvane AD Skopje\",\n",
        "    # \"Trudbenik AD Ohrid - dolgorocno suspendirano od kotacija\",\n",
        "    # \"Ugotur AD Skopje\", \"UNI Banka AD Skopje\", \"Vabtek MZT AD Skopje\",\n",
        "    # \"Veteks AD Veles\", \"ZAS AD Skopje\", \"Zito Karaorman AD Kicevo\",\n",
        "    # \"Zito Polog AD Tetovo\"\n",
        "]\n",
        "\n",
        "\n",
        "def filter_stockname():\n",
        "    stock_names = []\n",
        "    for company in stocks:\n",
        "        parts = company.split()\n",
        "        # get rid of city names for better search\n",
        "        filtered = [part for part in parts if\n",
        "                    part not in {\"AD\", \"Skopje\", \"Bitola\", \"Tetovo\", \"Prilep\", \"Kavadarci\", \"Ohrid\", \"Veles\",\n",
        "                                 \"Kumanovo\", \"Sveti\", \"Nikole\", \"Debar\", \"Radovis\", \"Stip\", \"Ilinden\", \"Kicevo\",\n",
        "                                 \"Strumica\", \" - dolgorocno suspendirano od kotacija\"}]\n",
        "        stock_names.append(\" \".join(filtered))\n",
        "\n",
        "    return stock_names\n",
        "\n",
        "\n",
        "def getStockCode_db():\n",
        "\n",
        "  stock_codes = [\n",
        "    # \"KMB\", \"ALK\", \"KOMU\", \"FERS\", \"GRNT\", \"MPOL\", \"INHO\", \"MTUR\", \"MKSD\",\n",
        "    \"MAKS\", \"MPT\", \"STIL\", \"REPL\", \"RZIT\", \"RZUS\", \"SPAZ\", \"SBT\", \"TETE\", \"TTK\", \"TKPR\",\n",
        "    \"VITA\", \"TKVS\", \"ZILU\", \"ZPKO\", \"ADIN\", \"AMEH\", \"APTK\", \"RZLE\", \"AUMK\", \"BIM\",\n",
        "    \"BLTU\", \"USJE\", \"CKB\", \"DEBA\", \"DIMI\",\n",
        "    # \"KPSS\", \"FAKM\", \"KONZ\", \"FUBT\",\n",
        "    # \"CEVI\", \"TIKV\", \"GECK\", \"GECT\", \"GRZD\", \"GTC\", \"INPR\", \"KLST\", \"BGOR\", \"RZLV\",\n",
        "    # \"LOTO\", \"KJUBI\", \"TEL\", \"MAKP\", \"MERM\", \"MODA\", \"MZPU\", \"NEME\", \"TNB\", \"NOSK\",\n",
        "    # \"OILK\", \"OKTA\", \"ORAN\", \"TRPS\", \"PKB\", \"LOZP\", \"POPK\", \"PPIV\", \"RADE\", \"BANA\",\n",
        "    # \"RZEK\", \"RZTK\", \"SSPR\", \"SIL\", \"SLAV\", \"SDOM\", \"STOK\", \"STB\", \"SPOL\", \"TAJM\",\n",
        "    # \"TEAL\", \"TEHN\", \"TSMP\", \"VROS\", \"TRDB\", \"RZUG\", \"UNI\", \"MZHE\", \"VTKS\", \"ZAS\",\n",
        "    # \"ZKAR\", \"ZPOG\"\n",
        "  ]\n",
        "\n",
        "\n",
        "  return stock_codes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnPPmOaiaTGq"
      },
      "outputs": [],
      "source": [
        "def create_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    return webdriver.Chrome(options=options)\n",
        "\n",
        "\n",
        "class SeleniumDriverPool:\n",
        "    def __init__(self, max_size=5):\n",
        "        self.pool = Queue(maxsize=max_size)\n",
        "        self.lock = threading.Lock()\n",
        "        for _ in range(max_size):\n",
        "            driver = create_driver()\n",
        "            self.pool.put(driver)\n",
        "\n",
        "    def acquire_driver(self):\n",
        "        with self.lock:\n",
        "            return self.pool.get()\n",
        "\n",
        "    def release_driver(self, driver):\n",
        "        with self.lock:\n",
        "            self.pool.put(driver)\n",
        "\n",
        "    def close_all_drivers(self):\n",
        "        while not self.pool.empty():\n",
        "            driver = self.pool.get()\n",
        "            driver.quit()\n",
        "\n",
        "\n",
        "pool = SeleniumDriverPool(max_size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqVcNd_BuN4x"
      },
      "source": [
        "<H1> SCRAPING <i>kapital.mk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_list(lst):\n",
        "    mid = len(lst) // 2\n",
        "    return lst[:mid], lst[mid:]"
      ],
      "metadata": {
        "id": "cK5jgAVXUk2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DhXo7E9EpHXw",
        "outputId": "b768b0f2-682b-4f03-aa9f-4e5dc690ab58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ТЕХНОЛОГИИ / КОМПАНИИ\n",
            "9 месеци ago\n",
            "ЕУ има амбиција да стане „квантна долина на светот“, а хрватски научници имаат голем удел во тоа\n",
            "Моќта на квантните компјутери допрва се испитува\n",
            "https://kapital.mk/%d0%bc%d0%b0%d1%80%d0%ba%d0%be-%d1%87%d0%b0%d0%b4%d0%b5%d0%b6-%d1%81%d1%80%d0%bf%d1%81%d0%ba%d0%b0-%d1%81%d1%82%d0%be%d0%bf%d0%b0%d0%bd%d1%81%d0%ba%d0%b0-%d0%ba%d0%be%d0%bc%d0%be%d1%80%d0%b0-%d0%bd/\n",
            "\n",
            "15.04.2024\n",
            "\n",
            "РЕГИОН\n",
            "9 месеци ago\n",
            "Марко Чадеж, Српска стопанска комора: Нашите компании ги мачи финансирањето и недостиг на кадри\n",
            "Српските компании со слични проблеми како и македонските\n",
            "https://kapital.mk/%d1%81%d1%83%d0%b4%d0%b1%d0%b8%d0%bd%d0%b0%d1%82%d0%b0-%d0%bd%d0%b0-%d1%84%d0%b0%d0%b1%d1%80%d0%b8%d0%ba%d0%b0%d1%82%d0%b0-%d0%b2%d0%be-%d0%b1%d1%83%d0%bd%d0%b0%d1%80%d1%9f%d0%b8%d0%ba-%d0%bd%d0%b5/\n",
            "\n",
            "15.04.2024\n",
            "\n",
            "МАКЕДОНИЈА\n",
            "9 месеци ago\n",
            "Судбината на фабриката во Бунарџик неизвесна, но можно е и целото производство да се префрли во Македонија\n",
            "Ван Хол, белгискиот производител на автобуси банкротираше\n",
            "https://kapital.mk/%d0%bc%d0%b0%d0%ba%d0%b5%d0%b4%d0%be%d0%bd%d1%81%d0%ba%d0%b8%d1%82%d0%b5-%d0%b5-%d1%82%d1%80%d0%b3%d0%be%d0%b2%d1%86%d0%b8-%d0%bd%d0%b0%d0%bf%d1%80%d0%b0%d0%b2%d0%b8%d0%bb%d0%b5-%d0%bf%d1%80%d0%be/\n",
            "\n",
            "15.04.2024\n",
            "\n",
            "МАКЕДОНИЈА\n",
            "9 месеци ago\n",
            "Македонските е-трговци направиле промет од 453 милиони евра лани\n",
            "Онлајн шопингот продолжува да расте со двоцифрена стапка\n",
            "https://kapital.mk/%d0%b7%d0%b5%d0%bb%d0%b5%d0%bd%d0%b8%d1%82%d0%b5-%d0%ba%d1%80%d0%b5%d0%b4%d0%b8%d1%82%d0%b8-%d1%81%d0%be-%d0%b3%d0%be%d0%b4%d0%b8%d1%88%d0%b5%d0%bd-%d1%80%d0%b0%d1%81%d1%82-%d0%be/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from datetime import datetime\n",
        "import threading\n",
        "from queue import Queue\n",
        "import os\n",
        "\n",
        "\n",
        "def scrape_kapitalMK():\n",
        "    stock_dataframes = {}\n",
        "    filtered_stocks, filtered_stocks_2 = split_list(filter_stockname())\n",
        "    stock_codes, stock_codes_2 = split_list(getStockCode_db())\n",
        "\n",
        "    pool1 = SeleniumDriverPool(max_size=20)\n",
        "    while True:\n",
        "        for stock_name, stock_code in zip(filtered_stocks, stock_codes):\n",
        "            data = []\n",
        "\n",
        "            try:\n",
        "                driver = pool1.acquire_driver()\n",
        "                i = 1\n",
        "\n",
        "                while True:\n",
        "                    driver.get(f'https://kapital.mk/page/{i}/?s={stock_name}')\n",
        "                    html_content = driver.page_source\n",
        "\n",
        "                    if 'Error 404!' in html_content or 'Sorry, your search did not match any entries.' in html_content:\n",
        "                        break\n",
        "\n",
        "                    article_list = driver.find_elements(By.CSS_SELECTOR, \".mvp-blog-story-list li\")\n",
        "\n",
        "                    for tag in article_list:\n",
        "                        a_tag = tag.find_element(By.CSS_SELECTOR, \"a:nth-child(1)\")\n",
        "                        link = a_tag.get_attribute('href')\n",
        "                        print(link)\n",
        "\n",
        "                        try:\n",
        "                            nd = pool1.acquire_driver()\n",
        "                            nd.get(link)\n",
        "                            date = nd.find_element(By.CSS_SELECTOR, 'time')\n",
        "                            date_published = datetime.fromisoformat(date.get_attribute('datetime')).strftime(\"%d.%m.%Y\")\n",
        "                            print()\n",
        "                            print(date_published)\n",
        "                            print()\n",
        "                            post = nd.find_element(By.CSS_SELECTOR, \".mvp-post-soc-out\").text\n",
        "\n",
        "                            actual_post = post.split('ПОВРЗАНИ ТЕМИ:')[0]\n",
        "                            # print(actual_post)\n",
        "\n",
        "                            title = a_tag.text\n",
        "                            print(title)\n",
        "                            data.append({'date': date_published, 'title': title, 'text': actual_post, 'link': link, 'from': 'kapital.mk'})\n",
        "\n",
        "                        except WebDriverException as e:\n",
        "                            print(f\"Error occurred while processing link {link}: {e}\")\n",
        "\n",
        "                        finally:\n",
        "                            pool1.release_driver(nd)\n",
        "\n",
        "                    i += 1\n",
        "\n",
        "            except WebDriverException as e:\n",
        "                print(f\"Error occurred for stock {stock_name} on page {i}: {e}\")\n",
        "\n",
        "            finally:\n",
        "                pool1.release_driver(driver)\n",
        "\n",
        "            stock_dataframes[stock_code] = pd.DataFrame(data)\n",
        "        break\n",
        "\n",
        "    pool1.close_all_drivers()\n",
        "\n",
        "    return stock_dataframes\n",
        "\n",
        "\n",
        "news2 = scrape_kapitalMK()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X9AVmQikv91"
      },
      "source": [
        "<h1>SCRAPING <i>biznisinfo.mk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmIfcSSDiViZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from datetime import datetime\n",
        "import threading\n",
        "from queue import Queue\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def scrape_biznisinfoMK():\n",
        "    stock_dataframes = {}\n",
        "    filtered_stocks, filtered_stocks_2 = split_list(filter_stockname())\n",
        "    stock_codes, stock_codes_2 = split_list(getStockCode_db())\n",
        "    pool2 = SeleniumDriverPool(max_size=20)\n",
        "    while True:\n",
        "\n",
        "        for stock_name, stock_code in zip(filtered_stocks, stock_codes):\n",
        "            data = []\n",
        "\n",
        "            try:\n",
        "                driver = pool2.acquire_driver()\n",
        "                i = 1\n",
        "\n",
        "                while True:\n",
        "                    driver.get(f'https://biznisinfo.mk/page/{i}/?s={stock_name}')\n",
        "                    html_content = driver.page_source\n",
        "\n",
        "                    if 'Упсссс... Error 404' in html_content or 'Нема резултати за вашето пребарување' in html_content:\n",
        "                        break\n",
        "\n",
        "                    article_list = driver.find_elements(By.CSS_SELECTOR, \".td_module_10\")\n",
        "                    print(f'Title: {article_list}')\n",
        "                    for tag in article_list:\n",
        "                        a_tag = tag.find_element(By.CSS_SELECTOR, \"div:nth-child(1) > a:nth-child(1)\")\n",
        "                        link = a_tag.get_attribute('href')\n",
        "                        print(link)\n",
        "\n",
        "                        try:\n",
        "                            nd = pool2.acquire_driver()\n",
        "                            nd.get(link)\n",
        "\n",
        "                            date = nd.find_element(By.CSS_SELECTOR, 'time')\n",
        "                            date_published = datetime.strptime(date.text, \"%d/%m/%Y\").strftime(\"%d.%m.%Y\")\n",
        "                            print()\n",
        "                            print(date_published)\n",
        "                            print()\n",
        "\n",
        "                            title = nd.find_element(By.CSS_SELECTOR, \"h1.entry-title\").text\n",
        "                            print(title)\n",
        "\n",
        "                            post = nd.find_element(By.CSS_SELECTOR, \".td-main-content\").text\n",
        "                            actual_post = post.split('Можеби ќе ве интересира и...')[0]\n",
        "\n",
        "                            # if stock_name in actual_post:\n",
        "                            data.append({'date': date_published, 'title': title, 'text': actual_post, 'link': link, 'from': 'biznisinfo.mk'})\n",
        "\n",
        "                        except WebDriverException as e:\n",
        "                            print(f\"Error occurred while processing link {link}: {e}\")\n",
        "\n",
        "                        finally:\n",
        "                            pool2.release_driver(nd)\n",
        "\n",
        "                    i += 1\n",
        "\n",
        "            except WebDriverException as e:\n",
        "                print(f\"Error occurred for stock {stock_name} on page {i}: {e}\")\n",
        "\n",
        "            finally:\n",
        "                pool2.release_driver(driver)\n",
        "\n",
        "            stock_dataframes[stock_code] = pd.DataFrame(data)\n",
        "\n",
        "        break\n",
        "\n",
        "    pool2.close_all_drivers()\n",
        "    return stock_dataframes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "news1 = scrape_biznisinfoMK()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pcy0EC9dya45"
      },
      "outputs": [],
      "source": [
        "for stock_name, df in news2.items():\n",
        "        print(f\"Data for {stock_name}:\")\n",
        "        print(df.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlOjqFv3b1QK"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ6F5p8c55wy"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from botocore.exceptions import ClientError\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "class WasabiClient:\n",
        "    def __init__(self):\n",
        "        access_key = 'A02ZUKINN1XDCJKJYI0D'\n",
        "        secret_key = 'ZB0o901t7NOFzmkmOU34HDFg2K1Pdrr0d3XKRhjh'\n",
        "        self.bucket =  'mkdstocks'\n",
        "        self.s3_client = boto3.client(\n",
        "            's3',\n",
        "            endpoint_url='https://s3.eu-central-2.wasabisys.com',\n",
        "            aws_access_key_id=access_key,\n",
        "            aws_secret_access_key=secret_key,\n",
        "        )\n",
        "\n",
        "    def fetch_data(self, key: str):\n",
        "        cloud_key = f\"Stock_Data/{key}.csv\"\n",
        "\n",
        "        try:\n",
        "            file_response = self.s3_client.get_object(Bucket=self.bucket, Key=cloud_key)\n",
        "            file_content = file_response['Body'].read().decode('utf-8')  # Fix typo\n",
        "            return pd.read_csv(io.StringIO(file_content))\n",
        "        except ClientError as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def update_or_create(self, code: str, new_df: pd.DataFrame):\n",
        "        cloud_key = f\"Stock_Data/{code}.csv\"\n",
        "\n",
        "        try:\n",
        "            try:\n",
        "                existing_file = self.s3_client.get_object(Bucket=self.bucket, Key=cloud_key)\n",
        "                existing_df = pd.read_csv(io.StringIO(existing_file['Body'].read().decode('utf-8')))\n",
        "            except ClientError as e:\n",
        "                if e.response['Error']['Code'] == 'NoSuchKey':\n",
        "                    existing_df = None\n",
        "                else:\n",
        "                    print(f\"Error fetching existing data: {e}\")\n",
        "                    return False\n",
        "\n",
        "            combined_df = pd.concat([new_df, existing_df]).drop_duplicates() if existing_df is not None else new_df\n",
        "            csv_buffer = io.StringIO()\n",
        "            combined_df.to_csv(csv_buffer, index=False)\n",
        "            csv_buffer.seek(0)\n",
        "            self.s3_client.put_object(Bucket=self.bucket, Key=cloud_key, Body=csv_buffer.getvalue())\n",
        "            print(f\"Successfully appended and uploaded data for {code}.\")\n",
        "        except ClientError as e:\n",
        "            print(f\"Error uploading data: {e}\")\n",
        "\n",
        "    def create_articles(self, code: str, df: pd.DataFrame):\n",
        "        cloud_key = f\"Articles/{code}.csv\"\n",
        "\n",
        "        try:\n",
        "            csv_buffer = io.StringIO()\n",
        "            df.to_csv(csv_buffer, index=False)\n",
        "            csv_buffer.seek(0)\n",
        "            self.s3_client.put_object(Bucket=self.bucket, Key=cloud_key, Body=csv_buffer.getvalue())\n",
        "            print(f\"Successfully appended and uploaded data for {code}.\")\n",
        "        except ClientError as e:\n",
        "            print(f\"Error uploading data: {e}\")\n",
        "\n",
        "    def fetch_articles(self, code: str):\n",
        "        cloud_key = f\"Articles/{code}.csv\"\n",
        "        try:\n",
        "            file_response = self.s3_client.get_object(Bucket=self.bucket, Key=cloud_key)\n",
        "            file_content = file_response['Body'].read().decode('utf-8')  # Fix typo\n",
        "            return pd.read_csv(io.StringIO(file_content))\n",
        "        except ClientError as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def get_wassabi_client():\n",
        "    global wassabi_client\n",
        "    if wassabi_client is None:\n",
        "        raise Exception(\"WasabiClient is not initialized. Check BackendConfig or initialization logic.\")\n",
        "    return wassabi_client\n",
        "\n",
        "\n",
        "# Setter for initialization\n",
        "def set_wassabi_client(client):\n",
        "    global wassabi_client\n",
        "    wassabi_client = client\n",
        "\n",
        "\n",
        "def initialize_wasabi_client():\n",
        "    return WasabiClient()\n",
        "\n",
        "\n",
        "wassabi_client = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *MODELS AND SENTIMENT ANALYSIS*"
      ],
      "metadata": {
        "id": "fFF3xUc_QfI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y6Nr4U7qB51"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer, pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer_finbert = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model_finbert = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\",\n",
        "                                                                   output_hidden_states=False)\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "# for translation\n",
        "tokenizer_translator = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mk-en')\n",
        "model_translator = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-mk-en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIlvcYg-6_bI"
      },
      "outputs": [],
      "source": [
        "def split_text_into_chunks(text, max_length):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        if len(' '.join(current_chunk + [word])) <= max_length:\n",
        "            current_chunk.append(word)\n",
        "        else:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "wasabi_client = WasabiClient()\n",
        "\n",
        "for stock_name, df in news2.items():\n",
        "  final_df = []\n",
        "  for index, row in df.iterrows():\n",
        "    # title\n",
        "    macedonian_title = row['title'].replace('„', '').replace('“', '')\n",
        "    encoded_input = tokenizer_translator(macedonian_title, return_tensors='pt')\n",
        "    translated = model_translator.generate(**encoded_input)\n",
        "    english_title = tokenizer_translator.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "    final_df.append({'date': row['date'], 'title': english_title, 'link': row['link']})\n",
        "\n",
        "    # content - cant exceed 512 tokens\n",
        "    macedonian_content = row['text']\n",
        "    chunks = split_text_into_chunks(macedonian_content, max_length=512)\n",
        "    translated_chunks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      encoded_input2 = tokenizer_translator(chunk, return_tensors='pt')\n",
        "      translated2 = model_translator.generate(**encoded_input2)\n",
        "      translated_chunk = tokenizer_translator.decode(translated2[0], skip_special_tokens=True)\n",
        "\n",
        "      result = sentiment_analyzer(translated_chunk)\n",
        "      print(result[0]['label'], result[0]['score'])\n",
        "      print()\n",
        "\n",
        "      final_df.append({f'Chunk: {i}': translated_chunk, f'Sentiment: {i}': result[0]['label']})\n",
        "\n",
        "      # Combine the translated chunks\n",
        "      english_content = ' '.join(translated_chunks)\n",
        "\n",
        "      # Print translated results\n",
        "      print(f\"Title (English): {english_title}\")\n",
        "      print(f\"Content (English): {english_content}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    # Optional: Save the translations to WasabiClient\n",
        "  final_for_stock = pd.DataFrame(final_df)\n",
        "  wasabi_client.create_articles(stock_name, final_for_stock)\n",
        "\n",
        "# Example fetching articles\n",
        "# wasabi_client.fetch_articles('Alkaloid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQAFjwIl9j6L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}